{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NESdYY9xLXU"
   },
   "source": [
    "# Graph Construction For Tweets\n",
    "\n",
    "This notebook download and process the Semeval Tweet data to build some possible graphs from the Tweets, in order to check the use of the GCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XARINOH9xeq_"
   },
   "source": [
    "## Data Download\n",
    "\n",
    "The first step is to download the processed data (that is already divided in train/test/validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWkcbIgfyJpW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval.abortion.test.csv\n",
      "semeval.abortion.train.csv\n",
      "semeval.abortion.validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100 44737  100 44737    0     0   117k      0 --:--:-- --:--:-- --:--:--  117k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf data/\n",
    "mkdir data/\n",
    "\n",
    "curl -LO https://cs.famaf.unc.edu.ar/~ccardellino/resources/semeval/semeval.abortion.tgz\n",
    "tar xvf semeval.abortion.tgz -C data/\n",
    "rm -rf semeval.abortion.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUAjky3NyWYv"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "Now that we have the data we need to process it. First of all, import the necessary libraries and continue with the loading of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RfkuXYqawqer",
    "outputId": "479c3df0-4d63-417f-9a1d-a9a75da3c63d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/crscardellino/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from gensim import corpora, models\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "PUNCTUATION = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "5TCTgsVjylJE",
    "outputId": "51b7591c-1805-464d-fe8a-b39a9059cc03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just laid down the law on abortion in my bioet...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bad 2 days for #Kansas Conservatives #ksleg @g...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Now that there's marriage equality, can we sta...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll always put all my focus and energy toward...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@BarackObama celebrates \"equality\" while 3000 ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              Tweet   Stance  Split\n",
       "0   0  Just laid down the law on abortion in my bioet...  AGAINST  Train\n",
       "1   1  Bad 2 days for #Kansas Conservatives #ksleg @g...     NONE  Train\n",
       "2   2  Now that there's marriage equality, can we sta...  AGAINST  Train\n",
       "3   3  I'll always put all my focus and energy toward...  AGAINST  Train\n",
       "4   4  @BarackObama celebrates \"equality\" while 3000 ...  AGAINST  Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset.append(\n",
    "        pd.read_csv(\"./data/semeval.abortion.{}.csv\".format(split))\n",
    "    )\n",
    "    dataset[-1].loc[:, \"Split\"] = split.capitalize()\n",
    "\n",
    "dataset = pd.concat(dataset, ignore_index=True)\n",
    "dataset.insert(0, \"ID\", dataset.index)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5C4XlY_yqwY"
   },
   "source": [
    "## Tweets Graph\n",
    "\n",
    "For this part I focus on the graph building cycle. Let's use the \"TweetTokenizer\" of NLTK for tokenizing the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "8yKZDP3kywHZ",
    "outputId": "c30b16f3-9fa6-4699-bb99-2981a5a6da9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Split</th>\n",
       "      <th>TokenizedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just laid down the law on abortion in my bioet...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Just, laid, down, the, law, on, abortion, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bad 2 days for #Kansas Conservatives #ksleg @g...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Bad, 2, days, for, #Kansas, Conservatives, #k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Now that there's marriage equality, can we sta...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Now, that, there's, marriage, equality, ,, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll always put all my focus and energy toward...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[I'll, always, put, all, my, focus, and, energ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@BarackObama celebrates \"equality\" while 3000 ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[@BarackObama, celebrates, \", equality, \", whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              Tweet   Stance  Split  \\\n",
       "0   0  Just laid down the law on abortion in my bioet...  AGAINST  Train   \n",
       "1   1  Bad 2 days for #Kansas Conservatives #ksleg @g...     NONE  Train   \n",
       "2   2  Now that there's marriage equality, can we sta...  AGAINST  Train   \n",
       "3   3  I'll always put all my focus and energy toward...  AGAINST  Train   \n",
       "4   4  @BarackObama celebrates \"equality\" while 3000 ...  AGAINST  Train   \n",
       "\n",
       "                                      TokenizedTweet  \n",
       "0  [Just, laid, down, the, law, on, abortion, in,...  \n",
       "1  [Bad, 2, days, for, #Kansas, Conservatives, #k...  \n",
       "2  [Now, that, there's, marriage, equality, ,, ca...  \n",
       "3  [I'll, always, put, all, my, focus, and, energ...  \n",
       "4  [@BarackObama, celebrates, \", equality, \", whi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"TokenizedTweet\"] = dataset[\"Tweet\"].apply(lambda t: casual_tokenize(t, reduce_len=True))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ru-ISWETy19I"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "We need to define a function that normalizes a token. Some posible normalizations I could think of:\n",
    "\n",
    "1. Lowercase.\n",
    "1. Remove hashtags/mentions.\n",
    "1. Normalize hashtags/mentions (e.g. by removing the \"#\"/\"@\" symbols). \n",
    "  1. This could be further expand by splitting the hashtags into multiple words.\n",
    "1. Remove punctuation.\n",
    "1. Remove stopwords (careful since many stopwords denote sentiment).\n",
    "1. Stemming.\n",
    "1. Remove low occurring words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ufQ2z7yy472"
   },
   "outputs": [],
   "source": [
    "def normalize_token(token, **kwargs):\n",
    "    if kwargs.get(\"remove_hashtags\") and token.startswith(\"#\"):\n",
    "        return \"\"\n",
    "    \n",
    "    if kwargs.get(\"remove_mentions\") and token.startswith(\"@\"):\n",
    "        return \"\"\n",
    "  \n",
    "    if kwargs.get(\"normalize_hashtags\") and token.startswith(\"#\"):\n",
    "        # TODO: Maybe a way to split hashtags?\n",
    "        token = token[1:]\n",
    "  \n",
    "    if kwargs.get(\"normalize_mentions\") and token.startswith(\"@\"):\n",
    "        token = token[1:]\n",
    "  \n",
    "    if kwargs.get(\"lowercase\"):\n",
    "        token = token.lower()\n",
    "  \n",
    "    return token\n",
    "\n",
    "\n",
    "def normalize_tweet(tweet, stopwords=set(), punctuation=set(), **kwargs):\n",
    "    tweet = [normalize_token(t, **kwargs).strip() for t in tweet \n",
    "             if t not in stopwords and t not in punctuation]\n",
    "  \n",
    "    return [t for t in tweet if t != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "2aFfT5rny7u1",
    "outputId": "fe3f4c58-a0ad-45ff-a079-da995ebbca68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TokenizedTweet</th>\n",
       "      <th>NormalizedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Just, laid, down, the, law, on, abortion, in,...</td>\n",
       "      <td>[just, laid, down, the, law, on, abortion, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Bad, 2, days, for, #Kansas, Conservatives, #k...</td>\n",
       "      <td>[bad, 2, days, for, conservatives, going, 0-4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Now, that, there's, marriage, equality, ,, ca...</td>\n",
       "      <td>[now, that, there's, marriage, equality, can, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[I'll, always, put, all, my, focus, and, energ...</td>\n",
       "      <td>[i'll, always, put, all, my, focus, and, energ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@BarackObama, celebrates, \", equality, \", whi...</td>\n",
       "      <td>[celebrates, equality, while, 3000, unborn, ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      TokenizedTweet  \\\n",
       "0  [Just, laid, down, the, law, on, abortion, in,...   \n",
       "1  [Bad, 2, days, for, #Kansas, Conservatives, #k...   \n",
       "2  [Now, that, there's, marriage, equality, ,, ca...   \n",
       "3  [I'll, always, put, all, my, focus, and, energ...   \n",
       "4  [@BarackObama, celebrates, \", equality, \", whi...   \n",
       "\n",
       "                                     NormalizedTweet  \n",
       "0  [just, laid, down, the, law, on, abortion, in,...  \n",
       "1  [bad, 2, days, for, conservatives, going, 0-4,...  \n",
       "2  [now, that, there's, marriage, equality, can, ...  \n",
       "3  [i'll, always, put, all, my, focus, and, energ...  \n",
       "4  [celebrates, equality, while, 3000, unborn, ba...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization_config = {\n",
    "    \"lowercase\": True,\n",
    "    \"remove_hashtags\": True,\n",
    "    \"remove_mentions\": True\n",
    "}\n",
    "\n",
    "dataset[\"NormalizedTweet\"] = dataset[\"TokenizedTweet\"].apply(\n",
    "    lambda t: normalize_tweet(t, punctuation=PUNCTUATION, **normalization_config)\n",
    ")\n",
    "\n",
    "dataset[[\"TokenizedTweet\", \"NormalizedTweet\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CrwjZGZAy_XG"
   },
   "source": [
    "For the TF-IDF we use gensim. Building a vocabulary from the corpus of Tweets, we remove all those tokens that are only present in one Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZCuaU1OzAmD"
   },
   "outputs": [],
   "source": [
    "tweet_vocab = corpora.Dictionary(dataset[\"NormalizedTweet\"])\n",
    "tweet_vocab.filter_extremes(no_below=2, no_above=1.0)\n",
    "\n",
    "bow_corpus = dataset[\"NormalizedTweet\"].apply(tweet_vocab.doc2bow).tolist()\n",
    "tfidf = models.TfidfModel(bow_corpus, dictionary=tweet_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Xh2gO40zKiG"
   },
   "source": [
    "### Graph edges\n",
    "\n",
    "This part is for creating the functions for extracting the features that will be in charge of creating the nodes of the graph.\n",
    "\n",
    "Some of the edges I thought of are:\n",
    "\n",
    "1. Hashtags overlap.\n",
    "1. Users's Mentions overlap.\n",
    "1. Ngrams overlap (3, 4, and 5 for now).\n",
    "1. Overlap of Top 10 TF-IDF words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXiNA3S8zUB8"
   },
   "outputs": [],
   "source": [
    "def extract_hashtags(tokens):\n",
    "    return sorted([t for t in tokens if t.startswith(\"#\") and t.strip() != \"#\"])\n",
    "\n",
    "def extract_mentions(tokens):\n",
    "    return sorted([t for t in tokens if t.startswith(\"@\") and t.strip() != \"@\"])\n",
    "\n",
    "def extract_ngrams(tokens, n=3):\n",
    "    return sorted([\"_\".join(ngram) for ngram in ngrams(tokens, n=n)])\n",
    "\n",
    "def extract_toptfdf(tweet, tfidf_model, vocab, k=10):\n",
    "    return sorted(tfidf_model[vocab.doc2bow(tweet)], key=itemgetter(1), reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3UAo6ATFzZO-",
    "outputId": "079fcd80-32d5-4db7-daa9-c3d7644caeee"
   },
   "outputs": [],
   "source": [
    "dataset[\"Hashtags\"] = dataset[\"TokenizedTweet\"].apply(extract_hashtags)\n",
    "dataset[\"Mentions\"] = dataset[\"TokenizedTweet\"].apply(extract_mentions)\n",
    "for i in range(2, 6):\n",
    "    dataset[\"{}-grams\".format(i)] = dataset[\"NormalizedTweet\"].apply(lambda t: extract_ngrams(t, n=i))\n",
    "dataset[\"TopTfIdf\"] = dataset[\"NormalizedTweet\"].apply(lambda t: extract_toptfdf(t, tfidf, tweet_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "bGjibsGszb4r",
    "outputId": "82e1582e-fc5c-47b5-901c-38c356be7cbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Split</th>\n",
       "      <th>TokenizedTweet</th>\n",
       "      <th>NormalizedTweet</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>2-grams</th>\n",
       "      <th>3-grams</th>\n",
       "      <th>4-grams</th>\n",
       "      <th>5-grams</th>\n",
       "      <th>TopTfIdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just laid down the law on abortion in my bioet...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Just, laid, down, the, law, on, abortion, in,...</td>\n",
       "      <td>[just, laid, down, the, law, on, abortion, in,...</td>\n",
       "      <td>[#Catholic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[abortion_in, bioethics_class, down_the, in_my...</td>\n",
       "      <td>[abortion_in_my, down_the_law, in_my_bioethics...</td>\n",
       "      <td>[abortion_in_my_bioethics, down_the_law_on, in...</td>\n",
       "      <td>[abortion_in_my_bioethics_class, down_the_law_...</td>\n",
       "      <td>[(1, 0.587697067393385), (2, 0.467890035647413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bad 2 days for #Kansas Conservatives #ksleg @g...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Bad, 2, days, for, #Kansas, Conservatives, #k...</td>\n",
       "      <td>[bad, 2, days, for, conservatives, going, 0-4,...</td>\n",
       "      <td>[#Kansas, #SCOTUSMarriage, #SCOTUScare, #SemST...</td>\n",
       "      <td>[@govsambrownback]</td>\n",
       "      <td>[0-4_in, 2_days, bad_2, conservatives_going, d...</td>\n",
       "      <td>[0-4_in_courts, 2_days_for, bad_2_days, conser...</td>\n",
       "      <td>[2_days_for_conservatives, bad_2_days_for, con...</td>\n",
       "      <td>[2_days_for_conservatives_going, bad_2_days_fo...</td>\n",
       "      <td>[(11, 0.5348717493397233), (10, 0.455119520052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Now that there's marriage equality, can we sta...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[Now, that, there's, marriage, equality, ,, ca...</td>\n",
       "      <td>[now, that, there's, marriage, equality, can, ...</td>\n",
       "      <td>[#SemST]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[can_we, equal_rights, equality_can, for_unbor...</td>\n",
       "      <td>[can_we_start, equal_rights_for, equality_can_...</td>\n",
       "      <td>[can_we_start_working, equal_rights_for_unborn...</td>\n",
       "      <td>[can_we_start_working_on, equal_rights_for_unb...</td>\n",
       "      <td>[(22, 0.4166756775234931), (24, 0.355166766759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll always put all my focus and energy toward...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[I'll, always, put, all, my, focus, and, energ...</td>\n",
       "      <td>[i'll, always, put, all, my, focus, and, energ...</td>\n",
       "      <td>[#SemST]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alive_instead, all_my, always_put, and_energy...</td>\n",
       "      <td>[alive_instead_of, all_my_focus, always_put_al...</td>\n",
       "      <td>[alive_instead_of_deciding, all_my_focus_and, ...</td>\n",
       "      <td>[alive_instead_of_deciding_who, all_my_focus_a...</td>\n",
       "      <td>[(32, 0.3120267165946575), (38, 0.312026716594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@BarackObama celebrates \"equality\" while 3000 ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Train</td>\n",
       "      <td>[@BarackObama, celebrates, \", equality, \", whi...</td>\n",
       "      <td>[celebrates, equality, while, 3000, unborn, ba...</td>\n",
       "      <td>[#LifeEquality, #SemST]</td>\n",
       "      <td>[@BarackObama]</td>\n",
       "      <td>[3000_unborn, a_real, about_a, babies_were, ce...</td>\n",
       "      <td>[3000_unborn_babies, a_real_inequality, about_...</td>\n",
       "      <td>[3000_unborn_babies_were, a_real_inequality_si...</td>\n",
       "      <td>[3000_unborn_babies_were_killed, about_a_real_...</td>\n",
       "      <td>[(45, 0.3833117705497533), (52, 0.383311770549...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              Tweet   Stance  Split  \\\n",
       "0   0  Just laid down the law on abortion in my bioet...  AGAINST  Train   \n",
       "1   1  Bad 2 days for #Kansas Conservatives #ksleg @g...     NONE  Train   \n",
       "2   2  Now that there's marriage equality, can we sta...  AGAINST  Train   \n",
       "3   3  I'll always put all my focus and energy toward...  AGAINST  Train   \n",
       "4   4  @BarackObama celebrates \"equality\" while 3000 ...  AGAINST  Train   \n",
       "\n",
       "                                      TokenizedTweet  \\\n",
       "0  [Just, laid, down, the, law, on, abortion, in,...   \n",
       "1  [Bad, 2, days, for, #Kansas, Conservatives, #k...   \n",
       "2  [Now, that, there's, marriage, equality, ,, ca...   \n",
       "3  [I'll, always, put, all, my, focus, and, energ...   \n",
       "4  [@BarackObama, celebrates, \", equality, \", whi...   \n",
       "\n",
       "                                     NormalizedTweet  \\\n",
       "0  [just, laid, down, the, law, on, abortion, in,...   \n",
       "1  [bad, 2, days, for, conservatives, going, 0-4,...   \n",
       "2  [now, that, there's, marriage, equality, can, ...   \n",
       "3  [i'll, always, put, all, my, focus, and, energ...   \n",
       "4  [celebrates, equality, while, 3000, unborn, ba...   \n",
       "\n",
       "                                            Hashtags            Mentions  \\\n",
       "0                                        [#Catholic]                  []   \n",
       "1  [#Kansas, #SCOTUSMarriage, #SCOTUScare, #SemST...  [@govsambrownback]   \n",
       "2                                           [#SemST]                  []   \n",
       "3                                           [#SemST]                  []   \n",
       "4                            [#LifeEquality, #SemST]      [@BarackObama]   \n",
       "\n",
       "                                             2-grams  \\\n",
       "0  [abortion_in, bioethics_class, down_the, in_my...   \n",
       "1  [0-4_in, 2_days, bad_2, conservatives_going, d...   \n",
       "2  [can_we, equal_rights, equality_can, for_unbor...   \n",
       "3  [alive_instead, all_my, always_put, and_energy...   \n",
       "4  [3000_unborn, a_real, about_a, babies_were, ce...   \n",
       "\n",
       "                                             3-grams  \\\n",
       "0  [abortion_in_my, down_the_law, in_my_bioethics...   \n",
       "1  [0-4_in_courts, 2_days_for, bad_2_days, conser...   \n",
       "2  [can_we_start, equal_rights_for, equality_can_...   \n",
       "3  [alive_instead_of, all_my_focus, always_put_al...   \n",
       "4  [3000_unborn_babies, a_real_inequality, about_...   \n",
       "\n",
       "                                             4-grams  \\\n",
       "0  [abortion_in_my_bioethics, down_the_law_on, in...   \n",
       "1  [2_days_for_conservatives, bad_2_days_for, con...   \n",
       "2  [can_we_start_working, equal_rights_for_unborn...   \n",
       "3  [alive_instead_of_deciding, all_my_focus_and, ...   \n",
       "4  [3000_unborn_babies_were, a_real_inequality_si...   \n",
       "\n",
       "                                             5-grams  \\\n",
       "0  [abortion_in_my_bioethics_class, down_the_law_...   \n",
       "1  [2_days_for_conservatives_going, bad_2_days_fo...   \n",
       "2  [can_we_start_working_on, equal_rights_for_unb...   \n",
       "3  [alive_instead_of_deciding_who, all_my_focus_a...   \n",
       "4  [3000_unborn_babies_were_killed, about_a_real_...   \n",
       "\n",
       "                                            TopTfIdf  \n",
       "0  [(1, 0.587697067393385), (2, 0.467890035647413...  \n",
       "1  [(11, 0.5348717493397233), (10, 0.455119520052...  \n",
       "2  [(22, 0.4166756775234931), (24, 0.355166766759...  \n",
       "3  [(32, 0.3120267165946575), (38, 0.312026716594...  \n",
       "4  [(45, 0.3833117705497533), (52, 0.383311770549...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTg_AIPvz2y4"
   },
   "source": [
    "### Graph Construction\n",
    "\n",
    "The graph constructions comes from the intersection of the extracted graph features of the previous part.\n",
    "\n",
    "For each type of edge, we define a graph. For now, we will use them as separate representations for different baselines. Eventually we can see how to aggregate all this info.\n",
    "\n",
    "This implementation uses brute force, not the best, but I'll check how to optimize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRWtIsd4z-Op"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   7 | elapsed:   46.2s remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   7 | elapsed:   46.8s remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:   47.4s remaining:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   7 | elapsed:   55.3s remaining:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "edges = [\"Hashtags\", \"Mentions\", \"TopTfIdf\"] + [\"{}-grams\".format(i) for i in range(2, 6)]\n",
    "\n",
    "def edge_adjacency_matrix(edge, dataset):\n",
    "    adjacency = []\n",
    "    for idx, row_i in dataset.iterrows():\n",
    "        adjacency.append((row_i[\"ID\"], row_i[\"ID\"], 0))  # Needed for NetworkX to keep track of all existing nodes (even isolated ones)\n",
    "        # We only store a triangular matrix (the matrix is symmetric)\n",
    "        for _, row_j in dataset.loc[idx+1:].iterrows():\n",
    "            edge_weight = len(set(row_i[edge]).intersection(row_j[edge]))  # TODO: Maybe weight this a little better?\n",
    "            if edge_weight > 0:\n",
    "                adjacency.append((row_i[\"ID\"], row_j[\"ID\"], edge_weight))\n",
    "    return edge, adjacency\n",
    "\n",
    "adjacencies = dict(\n",
    "    Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(edge_adjacency_matrix)(edge, dataset.loc[:, [\"ID\", edge]]) for edge in edges\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxxV52PH0F8r"
   },
   "source": [
    "## Save the data\n",
    "\n",
    "Finally, with both the graphs and the complete dataset (with the splits), we save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUUaWnia0K6x"
   },
   "outputs": [],
   "source": [
    "for edge, adjacency in adjacencies.items():\n",
    "    pd.DataFrame(\n",
    "        adjacency, \n",
    "        columns=[\"row\", \"col\", \"weight\"]\n",
    "    ).to_csv(\"./data/semeval.abortion.graph.{}.csv\".format(edge.lower()), index=False)\n",
    "\n",
    "dataset[[\"ID\", \"Tweet\", \"Stance\", \"Split\"]].to_csv(\"./data/semeval.abortion.data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXK0HDPH1KVi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval.abortion.data.csv\n",
      "semeval.abortion.graph.2-grams.csv\n",
      "semeval.abortion.graph.3-grams.csv\n",
      "semeval.abortion.graph.4-grams.csv\n",
      "semeval.abortion.graph.5-grams.csv\n",
      "semeval.abortion.graph.hashtags.csv\n",
      "semeval.abortion.graph.mentions.csv\n",
      "semeval.abortion.graph.toptfidf.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd data/\n",
    "tar zcvf semeval.abortion.graph_data.tgz semeval.abortion.data.csv semeval.abortion.graph.*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8Xqsgwq1Ymw"
   },
   "source": [
    "## Resource\n",
    "\n",
    "This resource is available at: https://cs.famaf.unc.edu.ar/~ccardellino/resources/semeval/semeval.abortion.graph_data.tgz"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "abortion_tweets_graph_construction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
